\documentclass[usenames,dvipsnames,10pt,compress, final, handout]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[]{graphicx}\usepackage[]{color}
 \usepackage{alltt} 
\let\Tiny=\tiny
\usepackage{eqnarray,amsmath}
% \usepackage{wasysym}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\usepackage{multirow}
% \usepackage{bigstrut}
\usepackage{graphicx}
\usepackage[round]{natbib}
\usepackage{bm}
\usepackage{tikzsymbols}

\setbeamertemplate{navigation symbols}{}    
\setbeamertemplate{footline}[frame number]{}
\usepackage{soul}
\usetheme{Singapore}

%Misc Commands
\newcommand{\mbf}{\mathbf}
\newcommand{\lexp}{$\overset{\mbox{\tiny 0}}{e}$}
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{5pt}}{\enditemize}


\newcommand{\bx}{{\bm x}}
\newcommand{\bX}{{\bm X}}
\newcommand{\by}{{\bm y}}
\newcommand{\bY}{{\bm Y}}
\newcommand{\bW}{{\bm W}}
\newcommand{\bG}{{\bm G}}
\newcommand{\bR}{{\bm R}}
\newcommand{\bZ}{{\bm Z}}
\newcommand{\bU}{{\bm U}}
\newcommand{\bV}{{\bm V}}
\newcommand{\bL}{{\bm L}}
\newcommand{\bz}{{\bm z}}
\newcommand{\be}{{\bm e}}
\newcommand{\bgamma}{{\bm \gamma}}
\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\balpha}{{\bm \alpha}}
\newcommand{\bSigma}{{\bm \Sigma}}
\newcommand{\bmu}{{\bm \mu}}
\newcommand{\btheta}{{\bm \theta}}
\newcommand{\bepsilon}{{\bm \epsilon}}
\newcommand{\bone}{{\bm 1}}
\newcommand{\bzero}{{\bm 0}}
\newcommand{\bC}{{\bm C}}
\newcommand{\bI}{{\bm I}}
\newcommand{\bA}{{\bm A}}
\newcommand{\bB}{{\bm B}}
\newcommand{\bQ}{{\bm Q}}
\newcommand{\bS}{{\bm S}}
\newcommand{\bD}{{\bm D}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\orange}{\textcolor{Orange}}
\newcommand{\green}{\textcolor{green}}
\newcommand{\blue}{\textcolor{blue}}
\newcommand{\red}{\textcolor{red}}
\newcommand{\purple}{\textcolor{purple}}
\newcommand{\gray}{\textcolor{gray}}
\newcommand{\ok}{\nonumber}
\newcommand{\argmin}{\mbox{argmin }}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

% Adjust vertical spacing in lists
\makeatletter
\def\@listi{\leftmargin\leftmargini
            \topsep 		8\p@ \@plus2\p@ \@minus2.5\p@
            \parsep 		0\p@
            \itemsep	5\p@ \@plus2\p@ \@minus3\p@}
\let\@listI\@listi
\def\@listii{\leftmargin\leftmarginii
              \topsep    6\p@ \@plus1\p@ \@minus2\p@
              \parsep    0\p@ \@plus\p@
              \itemsep  3\p@ \@plus2\p@ \@minus3\p@}
\def\@listiii{\leftmargin\leftmarginiii
              \topsep    3\p@ \@plus1\p@ \@minus2\p@
              \parsep    0\p@ \@plus\p@
              \itemsep  2\p@ \@plus2\p@ \@minus3\p@}
\makeatother
% Dealing with fraile envrionment of beamer with codes
\newenvironment{xframe}[2][]
  {\begin{frame}[fragile,environment=xframe,#1]
  \frametitle{#2}}
  {\end{frame}}


% trick to make all itemize pause
% \usepackage{letltxmacro}
% \LetLtxMacro\olditemize\itemize



\title{Stat 435 Intro to Statistical Machine Learning}
\subtitle{Week 6: Additional exercises}

\author[]{Richard Li}
\date{\today}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
% \renewcommand{\itemize}[1][<+(1)->]{\olditemize[#1]}

\maketitle
%================================================================%
\section{Ridge regression}
\stepcounter{subsection}
\frame{
  \frametitle{How do they compare}
  Assuming all $\bX$ and $\bY$ are centered,
  \begin{itemize}
    \item[] $\bbeta_1 = \argmin \norm{\bY - \bX\bbeta}^2_2$
    \item[] $\bbeta_2 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$
    \item[] $\bbeta_3 = \argmin \norm{\bY - a\bX\bbeta}^2_2$
    \item[] $\bbeta_4 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$
    \item[] $\bbeta_5 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + a^2\lambda\norm{\bbeta}^2_2$
    \item[] $\{\beta_0, \bbeta_6\} = \argmin \norm{(\bY + 1) - \beta_0 - (\bX+2)\bbeta}^2_2$
    \item[] $\{\beta_0, \bbeta_7\} = \argmin \norm{(\bY + 1) - \beta_0 - (\bX+2)\bbeta}^2_2+ \lambda\norm{\bbeta}^2_2$
  \end{itemize}

  \vspace{0.5cm}
  And what about the resulted RSS?
}
\frame{
  \frametitle{Least square: shifting by a constant}
  \begin{itemize}
    \item[] \red{$\bbeta_1 = \argmin \norm{\bY - \bX\bbeta}^2_2$}
    \item[] \red{$\{\beta_0, \bbeta_6\} = \argmin \norm{(\bY + 1) - \beta_0 - (\bX+2)\bbeta}^2_2$}
  \end{itemize}
  \pause
  \begin{itemize}
      \item \blue{$\bbeta_6 = \bbeta_1$}
      \item We can calculate $\beta_0$ from $\bbeta_6$, $\bar\bX$, and $\bar\bY$
      \item RSS is also the same
    \end{itemize}  
}

\frame{
  \frametitle{Least square: changing scales}
  \begin{itemize}
    \item[] \red{$\bbeta_1 = \argmin \norm{\bY - \bX\bbeta}^2_2$}
    \item[] \red{$\bbeta_3 = \argmin \norm{\bY - a\bX\bbeta}^2_2$}
  \end{itemize}
  \pause
  \begin{itemize}
      \item \blue{$\bbeta_3 = \bbeta_1 / a$}
      \item since $\bY - \bX\bbeta_1 = \bY - a\bX(\bbeta_1/a)$
      \item RSS is also the same
    \end{itemize}  
}
\frame{
  \frametitle{Ridge: shifting by a constant}
  \begin{itemize}
    \item[] \red{$\bbeta_2 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
    \item[] \red{$\{\beta_0, \bbeta_7\} = \argmin \norm{(\bY + 1) - \beta_0 - (\bX+2)\bbeta}^2_2+ \lambda\norm{\bbeta}^2_2$}
  \end{itemize}
  \pause
  \begin{itemize}
      \item \blue{$\bbeta_7 = \bbeta_2$}
      \item Notice $\beta_0$ is not penalized, so similar to the argument before
      \item We can calculate $\beta_0$ from $\bbeta_7$, $\bar\bX$, and $\bar\bY$
      \item RSS is also the same
    \end{itemize}  
}
\frame{
  \frametitle{Ridge: compare with least square}
  
  \begin{itemize}
    \item[]\red{$\bbeta_1 = \argmin \norm{\bY - \bX\bbeta}^2_2$}
    \item[]\red{$\bbeta_2 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
   \end{itemize}
    \pause
  \begin{itemize}
      \item For any single element: \blue{$|\beta_{2j}| \; ?\; |\beta_{1j}|$}, \textbf{not sure in general!}
          \begin{itemize}
            \item In some special cases, we know the relationship for sure, e.g.,  in class you derived the case when $\bX$ is identity matrix.
          \end{itemize}
      \pause    
      \item For the sum of squares \blue{$\norm{\bbeta_{2}}^2_2 \; <\; \norm{\bbeta_{1}}^2_2$}
          \begin{itemize}
            \item which implies when $p=1$, $|\beta_2| < |\beta_1|$.
          \end{itemize}
    \end{itemize}
}
\frame{
  \frametitle{Geometric intuition / alternative view}
  There is a one-to-one relationship between $\lambda$ and $s$ in
  \[
    \hat\bbeta_{ridge} = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2
  \] 
  \[
   \hat\bbeta_{ridge} = \argmin \norm{\bY - \bX\bbeta}^2_2 \;\;\; s.t. \;\; \norm{\bbeta}^2_2 \leq s 
  \]
  \vspace{3cm}

}

\frame{
  \frametitle{Proof 1: using SVD}
  In order to show this is true, we need to review something called \textbf{singular value decomposition (SVD)}.
  \begin{itemize}
    \item For any real matrix $\bX \in R^{n \times p}, n \geq p$, we can write 
    \item $\bX = \bU\bD\bV^T$, where
        \begin{enumerate}
          \item $\bU \in R^{n\times p}$ and $\bU^T\bU = \bm I_p$,
          \item $\bD \in R^{p \times p}$ and is diagonal,
          \item $\bV \in R^{p \times p}$ and $\bV^{T} = \bV^{-1}$.
        \end{enumerate}
  \end{itemize}
}

\frame{
  \frametitle{Proof 1: Plug in SVD}
  $\hat\bbeta_{ridge} = $
  \vspace{5cm}
}
\frame{
  \frametitle{Proof 1: Plug in SVD}
  $\norm{\hat\bbeta_{ridge}}^2_2 = \hat\bbeta_{ridge}^T\hat\bbeta_{ridge} =$
  \vspace{5cm}
}
\frame{
  \frametitle{Proof 2: By definition}
  Consider the general case with $\lambda_1 < \lambda_2$ \textit{\gray{(in the previous definition, $\lambda_1 = 0$, $\lambda_2 = \lambda$)}}
  \begin{itemize}
    \item[]$\bbeta_1 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda_1\norm{\bbeta}^2_2$
    \item[]$\bbeta_2 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda_2\norm{\bbeta}^2_2$
  \end{itemize}
  Now by definition, 
  \[
    \norm{\bY - \bX\bbeta_1}^2_2 + \red{\lambda_1}\norm{\bbeta_1}^2_2 \leq \norm{\bY - \bX\bbeta_2}^2_2 + \red{\lambda_1}\norm{\bbeta_2}^2_2
  \]
   \[
    \norm{\bY - \bX\bbeta_2}^2_2 + \red{\lambda_2}\norm{\bbeta_2}^2_2 \leq \norm{\bY - \bX\bbeta_1}^2_2 + \red{\lambda_2}\norm{\bbeta_1}^2_2
  \]
  Adding the two inequalities together, 
  \vspace{2cm}


}
\frame{
  \frametitle{Proof 2: By definition}
  Now plug in $\norm{\bbeta_1}^2 > \norm{\bbeta_2}^2$ back in to 
  \[
    \norm{\bY - \bX\bbeta_1}^2_2 + \red{\lambda_1}\norm{\bbeta_1}^2_2 \leq \norm{\bY - \bX\bbeta_2}^2_2 + \red{\lambda_1}\norm{\bbeta_2}^2_2
  \]
  \vspace{5cm}


}

\frame{
  \frametitle{Summary so far}
  So far we have shown that for ridge regression
  \begin{itemize}
    \item When we \red{increase $\lambda$, $\norm{\bbeta}^2_2$ decreases.}
    \item But we cannot guarantee every element in $|\bbeta|$ decreases.
    \item When we \red{increase $\lambda$, RSS increases.}
  \end{itemize}

  Now this fact allows us to compare more complicated scenarios...
}



\frame{
  \frametitle{Ridge: changing scale and $\lambda$ accordingly}
  \begin{itemize}
   \item[] \red{$\bbeta_2 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
   \item[] \red{$\bbeta_5 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + a^2\lambda\norm{\bbeta}^2_2$}
  \end{itemize}
  By change of variable, if we let $\balpha = a \times \bbeta$ in the second problem, 
  \[
  \min_{\bbeta} \norm{\bY - a\bX\bbeta}^2_2 + a^2\lambda\norm{\bbeta}^2_2
  = \min_{\balpha} \norm{\bY - \bX\balpha}^2_2 + \lambda\norm{\balpha}^2_2
  \]
  Notice we know the minimizer of RHS is $\bbeta_2$, thus $\balpha = \bbeta_2$, or
  \[
    \blue{\bbeta_5 = \frac{1}{a}\bbeta_2}
  \]
  And RSS is the same.

}
\frame{
  \frametitle{Ridge: changing scale and $\lambda$ fixed}
  \begin{itemize}
    \item[] \red{$\bbeta_2 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
     \item[] \gray{$\bbeta_5 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + a^2\lambda\norm{\bbeta}^2_2$}
    \item[] \red{$\bbeta_4 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
  \end{itemize}
  Consider $a > 1$, using $\bbeta_5$ as a middle step,
  \begin{itemize}
    % \item In general, \blue{$\beta_{4j} \;?\; \beta_{5j} = \frac{1}{a}\beta_{2j}$}, we cannot be sure!
    \item \blue{$\norm{\bbeta_4}^2_2 > \norm{\bbeta_5}^2_2 = \frac{1}{a^2}\norm{\bbeta_2}^2_2$}
    \item which implies if $p=1$, $|\beta_4| > |\beta_5| = \frac{1}{a}|\beta_2|$
  \end{itemize}
  ($a < 1$ is just the opposite.)
}
\frame{
  \frametitle{}
  \begin{itemize}
    \item[] \red{$\bbeta_1 = \argmin \norm{\bY - \bX\bbeta}^2_2$}
    \item[] \red{$\bbeta_2 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
    \item[] \red{$\bbeta_3 = \argmin \norm{\bY - a\bX\bbeta}^2_2$}
    \item[] \red{$\bbeta_4 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
    \item[] \red{$\bbeta_5 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + a^2\lambda\norm{\bbeta}^2_2$}
    \item[] \red{$\{\beta_0, \bbeta_6\} = \argmin \norm{(\bY + 1) - \beta_0 - (\bX+2)\bbeta}^2_2$}
    \item[] \red{$\{\beta_0, \bbeta_7\} = \argmin \norm{(\bY + 1) - \beta_0 - (\bX+2)\bbeta}^2_2+ \lambda\norm{\bbeta}^2_2$}
  \end{itemize}
  So, in the simple case of $p=1$, and $a > 1$
  \[
  \blue{|\beta_7| = |\beta_2| = a |\beta_5| < a |\beta_4| < a |\beta_3| = |\beta_1| = |\beta_6|}
  \]
  If we replace $|\cdot|$ with $\norm{\cdot}^2_2$, it holds for $p > 1$ case.
}
\frame{
  \frametitle{How about RSS?}
  We already know $\blue{RSS_1 = RSS_3 = RSS_6}$:
  \begin{itemize}
    \item[] \blue{$\bbeta_1 = \argmin \norm{\bY - \bX\bbeta}^2_2$}
    \item[] \blue{$\bbeta_3 = \argmin \norm{\bY - a\bX\bbeta}^2_2$}
    \item[] \blue{$\{\beta_0, \bbeta_6\} = \argmin \norm{(\bY + 1) - \beta_0 - (\bX+2)\bbeta}^2_2$}
  \end{itemize}
   Remember RSS increases as $\lambda$ increases, we have $\blue{RSS_3} < \red{RSS_4} < \orange{RSS_5}$
  \begin{itemize}
    \item[] \blue{$\bbeta_3 = \argmin \norm{\bY - a\bX\bbeta}^2_2$}
    \item[] \red{$\bbeta_4 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
    \item[] \orange{$\bbeta_5 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + a^2\lambda\norm{\bbeta}^2_2$}
  \end{itemize}
  And we also have shown $\orange{RSS_2 = RSS_5 = RSS_7}$
  \begin{itemize}
    \item[] \orange{$\bbeta_2 = \argmin \norm{\bY - \bX\bbeta}^2_2 + \lambda\norm{\bbeta}^2_2$}
    \item[] \orange{$\bbeta_5 = \argmin \norm{\bY - a\bX\bbeta}^2_2 + a^2\lambda\norm{\bbeta}^2_2$}
    \item[] \orange{$\{\beta_0, \bbeta_7\} = \argmin \norm{(\bY + 1) - \beta_0 - (\bX+2)\bbeta}^2_2+ \lambda\norm{\bbeta}^2_2$}
  \end{itemize}
 
}
\frame{
  \frametitle{Questions/exercises for you}
  \begin{itemize}
    \item Derive using SVD that RSS can be written as 
        \[
          \bY^T\bY + \sum_{j=1}^p ((\frac{d_j^2}{d_j^2 + \lambda} - 1)^2 - 1) \bm a_j^2
        \]
        where $\bm a = \bU^T\bY$, and show this is an increasing function of $\lambda > 0$.
     \item Can you say something similar if we change all ridge regression into Lasso?    

  \end{itemize}
 
}
%================================================================%
\section{Ridge regression}
\stepcounter{subsection}
\frame{
  \frametitle{Derivation of lasso estimator when $\bX^T\bX = \bI$}
  To derive the lasso estimator, we first notice that  
  \begin{eqnarray} \ok
    \hat\bbeta^{(lasso)} &=& \argmin_\bbeta \{(\bY - \bX\bbeta)^T (\bY - \bX\bbeta) + \lambda\sum_{j=1}^p |\beta_j|\}     \\\ok
    &=& \argmin_\bbeta\{\bY^T\bY - 2\bbeta^T\bX^T\bY + \bbeta^T\bX^T\bX\bbeta + \lambda\sum_{j=1}^p|\beta_j|\}    \\\ok
    &=& \argmin_\bbeta\{\bY^T\bY - 2\bbeta^T\bX^T\bY + \bbeta^T\bbeta + \lambda\sum_{j=1}^p|\beta_j|\}    \\\ok
    &=&\argmin_\bbeta\{\sum_{i=1}^n y_i^2 + \sum_{j=1}^p (\beta_j^2 + \lambda |\beta_j| - 2\beta_j\sum_{i=1}^n x_{ij}y_i)\}
  \end{eqnarray}

}
\frame{
  \frametitle{Derivation of lasso estimator when $\bX^T\bX = \bI$}
  The above objective function allows us to optimize each element of $\bbeta$ separately. Similar to what we have derived in class, we can see the minimum is achieved at
  \[
    \hat\beta_j = \max\{\sum_{i=1}^n x_{ij}y_i - \frac{\lambda}{2}, 0\} \;\;\;\; \mbox{ if }\;\; \hat\beta_j > 0
  \]
  \[
    \hat\beta_j = \min\{\sum_{i=1}^n x_{ij}y_i + \frac{\lambda}{2}, 0\} \;\;\;\; \mbox{ if }\;\; \hat\beta_j < 0
  \]
  which gives us the lasso solution
  \[ \hat\bbeta^{(lasso)} =
    \begin{cases}
      \sum_{i=1}^n x_{ij}y_i - \frac{\lambda}{2}      & \quad \text{if }  \sum_{i=1}^n x_{ij}y_i > \frac{\lambda}{2}\\
      \sum_{i=1}^n x_{ij}y_i + \frac{\lambda}{2}  & \quad \text{if } \sum_{i=1}^n x_{ij}y_i < -\frac{\lambda}{2}\\
      0  & \quad \text{otherwise}\\
    \end{cases}
  \]

}

%================================================================%
\section{PCA}
\stepcounter{subsection}
\frame{
  \frametitle{Motivation of PCA}
  \centering
  \includegraphics[width = 1\textwidth]{figure/PCA0.png}
}

\frame{
\frametitle{Geometric view of PCA}
 If we think of PCA as maximizing variances,\vspace{.5cm}

 PCA \textbf{rotates} the data set so as to align the directions in which it is spread out the most with the principal axes.

 \vspace{1cm}

 \textit{Remember in your previous HW, you found it difficult to visualize your classifier when there are more than two predictors, and you scratched your head trying to find two predictors that give you the best visual separation \st{(or you just randomly plotted something)}.}

 \vspace{1cm}
 A nice 3D visualization of the rotation: \url{http://setosa.io/ev/principal-component-analysis/}
}

\frame{
  \frametitle{Alternative view of PCA}
  (10.2.2 of ISLR) If we think of PCA as minimizing residuals, \\
  \centering
  \includegraphics[width = 1\textwidth]{figure/PCA1.png}
}
\frame{
  \frametitle{Alternative view of PCA}
  {\centering
  \includegraphics[width = 1\textwidth]{figure/PCA3.png}
  }

  This sequence matrices explain less and less variation in the data.\\
  \red{What happens when there are $p$ of them? Try the codes in 10.4 of ISLR, see if it is true.}
}


 
\end{document}
